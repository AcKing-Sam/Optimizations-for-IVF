import numpy as np
import time
import os
import struct
import csv

# ===================== é…ç½® =====================
source = '/data/vector_datasets/'
datasets = ['tiny5m', 'sift', 'gist']

K = 1024           # number of clusters
topK = 100         # Recall@100
nprobe_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100]

# =================================================


def read_vecs_fast(filename, show_progress=True):
    """
    å¿«é€Ÿè¯»å– .vecs æ ¼å¼çš„å‘é‡æ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°æ®å¹¶é‡æ–°ç»„ç»‡ï¼Œæ¯”é€ä¸ªå‘é‡è¯»å–å¿«å¾ˆå¤š
    
    å‚æ•°:
        filename: æ–‡ä»¶è·¯å¾„
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¯»å–è¿›åº¦
    """
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åæ¨æ–­æ•°æ®ç±»å‹
    if filename.endswith(".fvecs"):
        dtype = np.float32
        dtype_size = 4
    elif filename.endswith(".ivecs"):
        dtype = np.int32
        dtype_size = 4
    elif filename.endswith(".bvecs"):
        dtype = np.uint8
        dtype_size = 1
    else:
        raise ValueError(f"æœªçŸ¥çš„ vecs æ–‡ä»¶ç±»å‹: {filename}")
    
    if show_progress:
        print("  ğŸ“Š åˆ†ææ–‡ä»¶ç»“æ„...")
    
    # è·å–æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(filename)
    
    with open(filename, "rb") as f:
        # è¯»å–ç¬¬ä¸€ä¸ªå‘é‡çš„ç»´åº¦
        dim = struct.unpack('i', f.read(4))[0]
        
        # è®¡ç®—æ¯ä¸ªå‘é‡å ç”¨çš„å­—èŠ‚æ•°ï¼š4å­—èŠ‚(ç»´åº¦) + dim * dtype_size
        vec_size = 4 + dim * dtype_size
        
        # è®¡ç®—æ€»å‘é‡æ•°
        n = file_size // vec_size
        
        if show_progress:
            print(f"  ğŸ“ æ£€æµ‹åˆ° {n:,} ä¸ªå‘é‡ï¼Œæ¯ä¸ªç»´åº¦ {dim}")
            print(f"  ğŸ’¾ æ–‡ä»¶å¤§å°: {file_size / (1024**3):.2f} GB")
            print(f"  ğŸš€ å¼€å§‹å¿«é€Ÿè¯»å–...")
        
        # å›åˆ°æ–‡ä»¶å¼€å¤´
        f.seek(0)
        
        # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°æ®
        all_data = np.fromfile(f, dtype=np.uint8, count=file_size)
    
    if show_progress:
        print(f"  ğŸ”„ é‡ç»„æ•°æ®ç»“æ„...")
    
    # é«˜æ•ˆæ–¹æ³•ï¼šä½¿ç”¨numpyçš„è§†å›¾å’Œåˆ‡ç‰‡æ“ä½œï¼Œé¿å…Pythonå¾ªç¯
    # å°†å­—èŠ‚æ•°æ®é‡æ–°è§£é‡Šä¸ºç»“æ„åŒ–æ•°ç»„
    all_data = all_data.reshape(n, vec_size)
    
    # è·³è¿‡æ¯ä¸ªå‘é‡å‰4å­—èŠ‚çš„ç»´åº¦ä¿¡æ¯ï¼Œæå–å‘é‡æ•°æ®
    # all_data[:, 4:] è·³è¿‡å‰4åˆ—ï¼ˆç»´åº¦ä¿¡æ¯ï¼‰
    vec_data = all_data[:, 4:].copy()  # copy()ç¡®ä¿æ•°æ®è¿ç»­
    
    # å°†å­—èŠ‚æ•°æ®é‡æ–°è§£é‡Šä¸ºç›®æ ‡æ•°æ®ç±»å‹
    vectors = np.frombuffer(vec_data.tobytes(), dtype=dtype).reshape(n, dim)
    
    if show_progress:
        print(f"  âœ… å®Œæˆï¼è¯»å–äº† {n:,} ä¸ª {dim} ç»´å‘é‡")
    
    return vectors


def compute_recall(gt, I, topK):
    correct = 0
    for q in range(len(gt)):
        correct += np.intersect1d(gt[q][:topK], I[q]).size
    return correct / (len(gt) * topK)


def compute_l2_distances(X, Y):
    """è®¡ç®—ä¸¤ç»„å‘é‡ä¹‹é—´çš„ L2 è·ç¦»çŸ©é˜µ"""
    # X: (n, d), Y: (m, d)
    # è¿”å›: (n, m) è·ç¦»çŸ©é˜µ
    X_norm = np.sum(X ** 2, axis=1, keepdims=True)
    Y_norm = np.sum(Y ** 2, axis=1, keepdims=True).T
    distances = X_norm + Y_norm - 2 * np.dot(X, Y.T)
    # å¤„ç†æµ®ç‚¹è¯¯å·®ï¼Œç¡®ä¿è·ç¦»éè´Ÿ
    distances = np.maximum(distances, 0.0)
    return distances


def assign_to_clusters(vectors, centroids):
    """å°†å‘é‡åˆ†é…åˆ°æœ€è¿‘çš„ cluster"""
    distances = compute_l2_distances(vectors, centroids)
    assignments = np.argmin(distances, axis=1)
    return assignments


def build_inverted_lists(base_vectors, centroids):
    """æ„å»ºå€’æ’åˆ—è¡¨"""
    print("  Assigning vectors to clusters...")
    assignments = assign_to_clusters(base_vectors, centroids)
    
    print("  Building inverted lists...")
    # æ¯ä¸ª cluster å¯¹åº”ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«å±äºè¯¥ cluster çš„å‘é‡ç´¢å¼•
    inverted_lists = [[] for _ in range(K)]
    for vec_idx, cluster_id in enumerate(assignments):
        inverted_lists[cluster_id].append(vec_idx)
    
    # è½¬æ¢ä¸º numpy æ•°ç»„ä»¥ä¾¿åç»­ä½¿ç”¨
    list_lengths = [len(lst) for lst in inverted_lists]
    print(f"  Inverted lists built. Average list size: {np.mean(list_lengths):.1f}")
    
    return inverted_lists, assignments


def ivf_search(query_vectors, base_vectors, centroids, inverted_lists, nprobe, topK):
    """IVF æœç´¢ï¼šä½¿ç”¨æ‰‹åŠ¨æ„å»ºçš„å€’æ’åˆ—è¡¨"""
    # ä½¿ç”¨å‰100ä¸ªqueryï¼ˆå¦‚æœqueryæ•°é‡å°‘äº100ï¼Œåˆ™ä½¿ç”¨å…¨éƒ¨ï¼‰
    N_query = min(100, query_vectors.shape[0])
    N_base = base_vectors.shape[0]
    
    # å­˜å‚¨ç»“æœ
    I_out = np.zeros((N_query, topK), dtype=np.int32)
    D_out = np.zeros((N_query, topK), dtype=np.float32)
    
    # ç»Ÿè®¡è·ç¦»è®¡ç®—é‡
    dco = 0
    
    # å¯¹æ¯ä¸ªæŸ¥è¯¢å‘é‡
    for q_idx, q_vec in enumerate(query_vectors[:N_query]):
        # 1. æ‰¾åˆ°æœ€è¿‘çš„ nprobe ä¸ª centroids
        q_vec_expanded = q_vec[np.newaxis, :]  # (1, d)
        dists_to_centroids = compute_l2_distances(q_vec_expanded, centroids)[0]  # (K,)
        dco += K  # è®¡ç®—äº† K ä¸ªè·ç¦»åˆ° centroids
        
        # è·å–æœ€è¿‘çš„ nprobe ä¸ª cluster IDs
        candidate_clusters = np.argsort(dists_to_centroids)[:nprobe]
        
        # 2. æ”¶é›†å€™é€‰å‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨é›†åˆå»é‡ï¼Œå› ä¸ºå¯èƒ½åœ¨ä¸åŒçš„ cluster ä¸­æœ‰é‡å¤ï¼‰
        candidate_indices_set = set()
        for cluster_id in candidate_clusters:
            candidate_indices_set.update(inverted_lists[cluster_id])
        
        if len(candidate_indices_set) == 0:
            # å¦‚æœæ²¡æœ‰å€™é€‰ï¼Œè¿”å›ç©ºç»“æœ
            I_out[q_idx] = -1
            D_out[q_idx] = np.inf
            continue
        
        candidate_indices = np.array(list(candidate_indices_set), dtype=np.int32)
        candidate_vectors = base_vectors[candidate_indices]
        
        # 3. åœ¨å€™é€‰å‘é‡ä¸­æœç´¢
        dists_to_candidates = compute_l2_distances(q_vec_expanded, candidate_vectors)[0]  # (n_candidates,)
        dco += len(candidate_indices)  # è®¡ç®—äº† len(candidate_indices) ä¸ªè·ç¦»
        
        # 4. æ‰¾åˆ° topK
        top_k = min(topK, len(candidate_indices))
        top_indices_in_candidates = np.argsort(dists_to_candidates)[:top_k]
        top_indices = candidate_indices[top_indices_in_candidates]
        top_dists = dists_to_candidates[top_indices_in_candidates]
        
        # å¦‚æœå€™é€‰æ•°é‡å°‘äº topKï¼Œç”¨ -1 å’Œ inf å¡«å……
        if len(top_indices) < topK:
            padded_indices = np.full(topK, -1, dtype=np.int32)
            padded_dists = np.full(topK, np.inf, dtype=np.float32)
            padded_indices[:len(top_indices)] = top_indices
            padded_dists[:len(top_dists)] = top_dists
            I_out[q_idx] = padded_indices
            D_out[q_idx] = padded_dists
        else:
            I_out[q_idx] = top_indices
            D_out[q_idx] = top_dists
    
    return I_out, D_out, dco


# ===================== ä¸»å®éªŒ =====================
for dataset in datasets:
    print("="*80)
    print(f"IVF Search Performance - {dataset}")

    path = os.path.join(source, dataset)

    base = read_vecs_fast(os.path.join(path, f"{dataset}_base.fvecs"))
    query = read_vecs_fast(os.path.join(path, f"{dataset}_query.fvecs"))
    gt = read_vecs_fast(os.path.join(path, f"{dataset}_groundtruth.ivecs"))

    D = base.shape[1]
    
    # åªä½¿ç”¨å‰100ä¸ªquery
    n_queries_used = min(100, query.shape[0])
    print(f"Using first {n_queries_used} queries for evaluation")

    results = []

    # æ ¹æ®å½“å‰æ•°æ®é›†æ„å»ºè´¨å¿ƒæ–‡ä»¶å
    centroid_files = {
        "lloyd": f"centroids_lloyd_{dataset}.npy",
        "sgd": f"centroids_sgd_{dataset}.npy",
        "momentum": f"centroids_momentum_{dataset}.npy",
        "adam": f"centroids_adam_{dataset}.npy",
    }

    for method, fname in centroid_files.items():

        if not os.path.exists(fname):
            print(f"âš  Missing {fname}, skipping {method}")
            continue

        centroids = np.load(fname).astype('float32')
        
        # éªŒè¯ centroids æ•°é‡
        assert centroids.shape[0] == K, f"Centroids count mismatch: got {centroids.shape[0]}, expected {K}"

        print(f"\nBuilding IVF for {method}")

        # æ‰‹åŠ¨æ„å»ºå€’æ’åˆ—è¡¨
        inverted_lists, assignments = build_inverted_lists(base, centroids)

        # å¯¹æ¯ä¸ª nprobe å€¼è¿›è¡Œæœç´¢
        for nprobe in nprobe_list:
            print(f"  Searching with nprobe={nprobe}...")
            
            t0 = time.time()
            I_out, D_out, dco = ivf_search(query, base, centroids, inverted_lists, nprobe, topK)
            t1 = time.time()

            recall = compute_recall(gt[:n_queries_used], I_out, topK)
            qtime = (t1 - t0) / n_queries_used  # å¹³å‡æ¯ä¸ªæŸ¥è¯¢çš„æ—¶é—´ï¼ˆç§’ï¼‰
            qps = 1.0 / qtime if qtime > 0 else 0.0  # æ¯ç§’æŸ¥è¯¢æ•°

            print(f"[{method}] nprobe={nprobe} recall={recall:.4f} "
                  f"DCO={dco} time/query={qtime*1e3:.3f} ms QPS={qps:.2f}")

            results.append({
                "dataset": dataset,
                "method": method,
                "nprobe": nprobe,
                "recall": recall,
                "dco": dco,
                "time": qtime,
                "qps": qps,
            })

    # ä¿å­˜ç»“æœ
    out_csv = f"ivf_search_{dataset}.csv"
    with open(out_csv, "w", newline="") as fp:
        w = csv.DictWriter(fp, fieldnames=["dataset","method","nprobe",
                                           "recall","dco","time","qps"])
        w.writeheader()
        for r in results:
            w.writerow(r)

    print(f"âœ… Saved {out_csv}")
